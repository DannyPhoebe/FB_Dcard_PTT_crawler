{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests,re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "\n",
    "def page_load(url):\n",
    "    results = requests.get(url,headers=header,verify=True)\n",
    "    return json.loads(results.text)\n",
    "\n",
    "\n",
    "class FB_crawler(object):\n",
    "    def __init__(self, page_id, start, until, token, outfile, sleep=True):\n",
    "        self.page_id = page_id if type(page_id) == str else str(page_id)\n",
    "        self.start = start\n",
    "        self.until = until\n",
    "        self.token = token\n",
    "        self.outfile = outfile\n",
    "        self.sleep = sleep\n",
    "        self.post_ids = []\n",
    "        self.url_num = None\n",
    "        self.data = dict()\n",
    "        print(\"Facebook_crawler is activated...\")\n",
    "\n",
    "\n",
    "    def pid_collector(self):\n",
    "        query = '%s/posts?fields=name,link,full_picture,message&limit=100&since=%s&until=%s' % (self.page_id,self.start,self.until)\n",
    "        posts = page_load(\"https://graph.facebook.com/v2.8/%s&access_token=%s\"%(query, self.token))\n",
    "        while len(posts['data']) > 0:\n",
    "            self.post_ids.append(list(map(lambda x:x['id'],posts['data'])))\n",
    "            posts = page_load(posts['paging']['next'])\n",
    "        self.post_ids = sum(self.post_ids,[])\n",
    "        self.url_num = len(self.post_ids)\n",
    "        print(self.url_num,\"post urls are collected\")\n",
    "\n",
    "                                 \n",
    "    def crawler(self):\n",
    "        assert len(self.post_ids) > 0,'please run pid_collector first or reset the search period'\n",
    "        self.data['info'] = {'start':self.start,'until':self.until}\n",
    "        self.data['post'] = [None]*self.url_num\n",
    "        for p_idx,pid in enumerate(self.post_ids):\n",
    "            # start comment crawler\n",
    "            if p_idx % 20 == 0: print('crawling post {}...'.format(p_idx))\n",
    "            com_query = r'%s?fields=name,description,link,message,comments' % (pid)\n",
    "            com = page_load(\"https://graph.facebook.com/v2.8/%s&access_token=%s\" % (com_query, self.token))\n",
    "            self.data['post'][p_idx] = com\n",
    "            while True: # restart gate for comment crawler\n",
    "                try:\n",
    "                    while 'paging' in com and 'next' in com['paging']:\n",
    "                        com = page_load(com['paging']['next'])\n",
    "                        self.data['post'][p_idx]['comments']['data'].append(com['data'])\n",
    "                except:\n",
    "                    print('retry')\n",
    "                    time.sleep(5)\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "            # start reaction crawler\n",
    "            tmp = dict()\n",
    "            for idx,rxn_type in enumerate(['ANGRY','SAD','LOVE','HAHA','WOW','LIKE']):\n",
    "                rxn_query = '%s?fields=reactions.type(%s).summary(true)' % (pid,rxn_type)\n",
    "                rxn = page_load(\"https://graph.facebook.com/v2.8/%s&access_token=%s\" % (rxn_query, self.token)) \n",
    "                try:\n",
    "                    tmp[rxn_type] = rxn['reactions']['summary']['total_count']\n",
    "                except KeyError:\n",
    "                    tmp[rxn_type] = None\n",
    "            self.data['post'][p_idx]['reaction'] = tmp\n",
    "            if p_idx % 100 == 0:\n",
    "                time.sleep(5)\n",
    "            json.dump(self.data, open(self.outfile, 'w', encoding='utf-8'))\n",
    "        print('process complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook_crawler is activated...\n",
      "95 post urls are collected\n",
      "crawling post 0...\n",
      "crawling post 20...\n",
      "crawling post 40...\n",
      "crawling post 60...\n",
      "crawling post 80...\n",
      "process complete.\n"
     ]
    }
   ],
   "source": [
    "token='EAACEdEose0cBABlnG6WZB75AEKXsi6iRg2ZCn2QSk1hMkWINSCBbHZAZC2dh3ZCs0EMRt2A1Sl7YQzXpcQZAsQ4n1HWTHadWNpDZAvc6QEUsIcq4wkFPaEI9Q43z6nQNWPuI1uiBoL2o6AaAGuuNtqD3X8LtMnLnYRCoBHZCQGyiheIQq7ZCcCZAW3'\n",
    "c = FB_crawler('232633627068','17-01-01','17-01-02',token,'result.json')\n",
    "c.pid_collector()\n",
    "c.crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': '馬前總統嫁女兒也低調沒報、幹嘛特意要報蔡總統姪女出嫁？管很寬哦？！', 'created_time': '2017-01-01T08:30:14+0000', 'from': {'name': 'Bell Pan', 'id': '799692476717406'}, 'id': '1249935678408100_1249953408406327'}\n"
     ]
    }
   ],
   "source": [
    "print(c.data['post'][55][\"comments\"]['data'][1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
